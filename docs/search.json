[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Solutions to Statistical Computing with R (2ed) by Maria Rizzo",
    "section": "",
    "text": "Welcome\n\n\n\n\n\n\nWarning\n\n\n\nI have just begun to work through Rizzo’s book. This set of solutions is decidedly incomplete. Every aspect of my work is subject to change. Many things might be incorrect. Are you sure you want to read this yet?\n\n\n\n\nThis is a website that provides solutions to the exercises from Maria Rizzo’s book Statistical Computing in R (2ed).",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "chapters/01_intro.html",
    "href": "chapters/01_intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "Q1.1\nGenerate a random sample \\(x_1, \\ldots, x_{100}\\) of data from the \\(t_4\\) (df=4) distribution using the rt function. Use the MASS::truehist function to display a probability histogram of the sample.\n\nset.seed(1234)\nx &lt;- rt(n=100, df=4)\nMASS::truehist(x, las=1, col=\"#118ab2\", border=\"grey80\", nbins=20)\n\n\n\n\n\n\n\n\n\n\nQ1.2\nAdd the \\(t_4\\) density curve (dt) to your histogram in Exercise 1.1 using the curve function with add=TRUE.\n\nMASS::truehist(x, las=1, col=\"#118ab2\", border=\"grey80\", nbins=20)\ncurve(dt(x, df=4), from=-4, to=4, add=T, type=\"l\")\n\n\n\n\n\n\n\n\n\n\nQ1.3\nAdd an estimated density curve to your histogram in Exercise 1.2 using density.\n\nMASS::truehist(x, las=1, col=\"#118ab2\", border=\"grey80\", nbins=20)\ncurve(dt(x, df=4), from=-4, to=4, add=T, type=\"l\")\nlines(density(x), col=\"firebrick\", lwd=2)\n\n\n\n\n\n\n\n\n\n\nQ1.4a\nWrite an R function f in R to implement the function \\(f(x) = (x-a)/b\\) that will transform an imput vector \\(x\\) and return the result. The function should take three input arguments: x, a, and b.\n\nf &lt;- function(x, a, b) (x-a)/b\n\n\n\nQ1.4b\nTo transform \\(x\\) to the interval \\([0,1]\\) we subtract the minimum value and divice by the range: y &lt;- f(x, a=min(x), b=max(x)-min(x)). Generate a random sample of a Normal(\\(\\mu=2\\), \\(\\sigma=2\\)) data using rnorm and use your function f to transform this sample to the interval \\([0,1]\\). Print a summary of both the sample x and the transformed sample y to check the result.\n\nset.seed(1234)\nx &lt;- rnorm(100, mean=2, sd=2)\ny &lt;- f(x, min(x), max(x)-min(x))\nsummary(data.frame(x=x, y=y))\n\n       x                 y         \n Min.   :-2.6914   Min.   :0.0000  \n 1st Qu.: 0.2093   1st Qu.:0.2963  \n Median : 1.2307   Median :0.4007  \n Mean   : 1.6865   Mean   :0.4472  \n 3rd Qu.: 2.9424   3rd Qu.:0.5755  \n Max.   : 7.0980   Max.   :1.0000  \n\n\n\n\nQ1.5\nRefer to Exercise 1.4. Suppose that we want to transform the x sample so that it has mean zero and standard deviation one (studentize the sample). That is, we want \\(z_i - (x_i-\\bar{x})/s\\) for \\(i=1,\\ldots,n\\), where \\(s\\) is that standard deviation of the sample. Using your function f this is z &lt;- f(x, a=mean(x), b=sd(x)). Display a summary and histogram of the studentized sample z. It should be centered exactly at zero. Use sd(z) to check that the studentized sample has standard deviation exactly 1.0.\n\nz &lt;- f(x, a=mean(x), b=sd(x))\nround(mean(z),10); round(sd(z),10)\n\n[1] 0\n\n\n[1] 1\n\n\n\n\nQ1.6\nUsing your function f of Exercise 1.4, center and scale your Normal(\\(\\mu=2\\), \\(\\sigma=2\\)) sample by subtracting the sample median and dividing by the sample interquartile range (IQR).\n\ny &lt;- f(x, a=median(x), b=IQR(x))\nplot(z, y, pch=20)\n\n\n\n\n\n\n\n\n\n\nQ1.7\nRefer to Example 1.14 where we displayed an arrach of scatterplots using ggplot with facet_wrap. One of the variables in the mpg data is drv, a characer vector indicating whether the vehicle is front-wheel drive, real-wheel drive, or four-wheel drive. Add color=drv in aes: aes(displ, hwy, color=drv) and display the revised plot. Your scatterplots should now have automatically generated a legend for drv color.\n\nlibrary(ggplot2)\nmpg |&gt;\n    ggplot() + \n    geom_point(aes(displ, hwy, color=drv)) + \n    facet_wrap(~class)\n\n\n\n\n\n\n\n\n\n\nQ1.8\nThis exercise is intented to serve as an introduction to report writing with R Markdown. Install the knitr package if it is not installed. Create an html report using R Markdown and knitr in RStudio. The report whould include the code and output of Examples 1.12 and 1.14 with appropriate headings and a brief explanation o feach example.\nSolution Omitted.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/02_probstat.html",
    "href": "chapters/02_probstat.html",
    "title": "2  Probability and Statistics Review",
    "section": "",
    "text": "No questions.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Probability and Statistics Review</span>"
    ]
  },
  {
    "objectID": "chapters/03_genrv.html",
    "href": "chapters/03_genrv.html",
    "title": "3  Methods for Generating Random Variables",
    "section": "",
    "text": "Q3.1\nWrite a function that will generate and return a random sample of size \\(n\\) from the two-parameter exponential distribution Exp(\\(\\lambda\\),\\(\\eta\\)) for arbitrary \\(n\\), \\(\\lambda\\), and \\(\\eta\\). (See Examples 2.3 and 2.6.) Generate a large sample from Exp(\\(\\lambda\\),\\(\\eta\\)) and compare your sample quantiles with the theoretical quantiles.\n\n\nQ3.2\nThe standard Lapace distribution has density \\(f(x) = \\frac{1}{2}e^{-\\vert x \\vert}\\) for \\(x \\in \\mathbb{R}\\). Use the inverse transform method to generate a random sample of size 1000 from this distribution. Use one of the methods shown in this chapter to compare the generated sample to the target distribution.\n\n\nQ3.3\nThe Pareto(\\(a\\), \\(b\\)) distribution has cdf\n\\[ F(x) = 1 -  \\left(\\frac{b}{x}\\right)^a, \\hspace{1em} x \\ge b &gt; 0, \\; a &gt; 0 \\]\nDerive the probability inverse transformation \\(F^{-1}(U)\\) and use the inverse transform method to simulate a random sample from the Pareto(2,2) distribution. Graph the density histogram of the sample with the Pareto(2,2) density superimposed for comparison.\n\n\nQ3.4\nThe Rayleigh density is\n\\[ f(x) = \\frac{x}{\\sigma^2}e^{-x^2/(2\\sigma^2)}, \\hspace{1em} x \\ge 0, \\; \\sigma &gt; 0 \\]\nDevelop an algorithm to generate random samples from a Rayleigh(\\(\\sigma\\)) distribution. Generate Rayleigh(\\(\\sigma\\)) samples for several choices of \\(\\sigma &gt; 0\\) and check that the mode of the generated samples is close to the theoretical mode \\(\\sigma\\) (check the histogram).\n\n\nQ3.5\nA discrete random variable \\(X\\) has probability mass function\n[insert table]\nUse the inverse transform method to generate a random sample of size 1000 from the distribution of \\(X\\). Construct a relative frequency table and compare the empirical with the theoretical probabilities. Repeat using the R sample function.\n\n\nQ3.6\nProve that the accepted variates generated by the acceptance-rejection sampling algorithm are a random sample from the target density \\(f_X\\).\n\n\nQ3.7\nWrite a function to generate a random sample of size \\(n\\) from the Beta(\\(a\\), \\(b\\)) distribution by the acceptance-rejection method. Generate a random sample of size 1000 from the Beta(3,2) distribution. Graph the histogram of the sample with the theoretical Beta(3,2) density superimposed.\n\n\nQ3.8\nWrite a function to generate random variates from a Lognormal(\\(\\mu\\), \\(\\sigma\\)) distribution using a transformation method, and generate a random sample of size 1000. Compare the historgram with the lognormal density curve given by the dlnorm function in R.\n\n\nQ3.9\nThe rescaled Epanechnikov kernal is a symmetric density function\n\\[ f_e{x} = \\frac{3}{4}(1-x^2), \\hspace{1em} \\vert x \\vert \\le 1 \\]\nDevroye and Gyorfi give the following algorithm for simulation from this distribution. Generate iid \\(U_1, U_2, U_3 \\sim \\text{Uniform}(-1,1)\\). If \\(\\vert U_3 \\vert \\ge \\vert U_2 \\vert\\) and \\(\\vert U_3 \\vert \\ge \\vert U_1 \\vert\\), deliver \\(U_2\\); otherwise deliver \\(U_3\\). Write a function to generate random variates from \\(f_e\\), and construct the histrogram density estimate of a large simulated random sample.\n\n\nQ3.10\nProve that the algorithm given in Exercise 3.9 generates variates from the density \\(f_e\\).\n\n\nQ3.11\nGenerate a random sample of size 1000 from a normal location mixture. The components of the mixture have \\(N(0,1)\\) and \\(N(3,1)\\) distributions with mixing probabilities \\(p_1\\) and \\(p_2 = 1 - p_1\\). Graph the histogram of the sample with density superimposed, for \\(p_1 = 0.75\\). Repeat with different values for \\(p_1\\) and observe whether the empirical distribution of the mixture appears to be bimodal. Make a conjecture about the values of \\(p_1\\) that produce bimodal mixtures.\n\n\nQ3.12\nSimulate a continuous Exponential-Gamma mixture. Suppose that the rate parameter \\(\\Lambda\\) has Gamma(\\(r\\), \\(\\beta\\)) distribution and \\(Y\\) has Exp(\\(\\Lambda\\)) distribution. That is, \\((Y \\vert \\Lambda = \\lambda) \\sim f_Y(y \\vert \\lambda) = \\lambda e^{-\\lambda y}\\). Generate 1000 random observations from this mixture with \\(r=4\\) and \\(\\beta=2\\)\n\n\nQ3.13\nIt can be shown that the mixture in Exercise 3.12 has a Pareto distibution with cdf\n\\[ F(y) = 1 - \\left( \\frac{\\beta}{\\beta + y} \\right)^r, \\hspace{1em} y \\ge 0\\]\n(This is an alternative parameterization of the Paredo dcf given in Exercise 3.3.) Generate 1000 random observations from the mixture with \\(r=4\\) and \\(\\beta = 2\\). Compare the empirical tand theoretical (Pareto) distributions by graphing the density histogram of the sample and superimposing the Pareto density curve.\n\n\nQ3.14\nGenerate 200 random observations from the 3-dimensional multivariate normal distribution having mean vecter \\(\\mu = (0,1,2)\\) and covariance matrix\n\\[\n\\Sigma =\n\\begin{bmatrix}\n   1.0 & -0.5 &  0.5 \\\\\n  -0.5 &  1.0 & -0.5 \\\\\n   0.5 & -0.5 &  1.0\n\\end{bmatrix}\n\\]\nusing the Choleski factorization method. Use the R pairs plot to graph an arrach of scatter plots for each pair of variables. For each pair of variables (visually) check that the location and correlation approximately agree with the theoretical parameters of the correponding bivariate normal distribution.\n\n\nQ3.15\nWrite a function that will standardize a multivariate normal sample for arbitrary \\(n\\) and \\(d\\). That is, transform the sample so that the sample mean vectore is zero and sample covariance is the identity matrix. To check your results, generate multivariate normal samples and print the sample mean vector and covariance matrix before and after standardization.\n\n\nQ3.16\nEfron and Tibshirani discuss the scor (bootstrap) test score data on 88 students who took examizations in five subjects. Each row of the data frame is a set of scores \\((x_{i1}, \\ldots, x_{i5})\\) for the \\(i^{th}\\) student. Standardize the scores by type of exam. That is, standardize the bivariate samples (\\(X_1, X_2\\)) (closed book) and the trivariate samples (\\(X_3, X_4, X_5\\)) (open book). Compute the covariance matrix of the transformed sample of test scores.\n\n\nQ3.17\nCompare the performance of the Beta generator of Exercise 3.7, Example 3.8, and the R generator rbeta. Fix the parameters \\(a=2\\), \\(b=2\\) and time each generator of 1000 iterations with sample size 5000. (See Example 3.19.) Are the results different for different choices of \\(a\\) and \\(b\\)?\n\n\nQ3.18\nWrite a function to generate a random sample from a \\(W_d(\\Sigma,n)\\) (Wishart) distribution for \\(n &gt; d+1 \\ge 1\\), based on Bartlett’s decomposition.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Methods for Generating Random Variables</span>"
    ]
  },
  {
    "objectID": "chapters/04_genrp.html",
    "href": "chapters/04_genrp.html",
    "title": "4  Generating Random Processes",
    "section": "",
    "text": "Q4.1\nSuppose that A and B each start with a stake of $10, and bet $1 on consecutive coin flips. The game ends when either one of the players has all of the money. Let \\(S_n\\) be the fortune of player A at time \\(n\\). Then \\(\\left\\{ S_n, \\; n \\ge 0 \\right\\}\\) is a symmetric random walk with absorving barriers at 0 and 20. Simulate a realization of this process and plot \\(S_n\\) vs. the time index from time 0 until a barrier is reached.\n\n\nQ4.2\nA compound Poisson process is a stochastic process \\(\\left\\{ X(t), \\; t \\ge 0 \\right\\}\\) that can be represented as the random sum \\(X(t) = \\sum_{i=1}^{N(t)} Y_i, \\; t \\ge 0\\), where \\(\\left\\{ N(t), \\; t \\ge 0 \\right\\}\\) is a Poisson process and \\(Y_1, Y_2, \\ldots\\) are iid and independent of \\(\\left\\{ N(t), \\; t \\ge 0 \\right\\}\\). Write a program to simulate a compound Poisson(\\(\\lambda\\))-Gamma process (\\(Y\\) has a Gamma distribution). Estimate the mean and the variance of \\(X(10)\\) for several choices of the parameters and compare with the theoretical values. Hinkt: Show that \\(E[X(t)] = \\lambda t E[Y_1]\\) and \\(\\text{Var}\\left(X(t)\\right) = \\lambda t E[Y_1^2]\\).\n\n\nQ4.3\nA nonhomogeneous Poisson process has mean value function\n\\[ m(t) = t^2 + 2t, \\hspace{1em} t \\ge 0 \\]\nDetermine the intensity function \\(\\lambda(t)\\) of the process, and write a program to simulate the process on the interval [4,5]. Compute the probability distribution of \\(N(5)-N(4)\\), and compare it to the empirical estimate obtained by replicating the simulation.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Generating Random Processes</span>"
    ]
  },
  {
    "objectID": "chapters/05_viz.html",
    "href": "chapters/05_viz.html",
    "title": "5  Visualization of Multivariate Data",
    "section": "",
    "text": "Q5.1\n\n\nQ5.2\n\n\nQ5.3\n\n\nQ5.4\n\n\nQ5.5\n\n\nQ5.6\n\n\nQ5.7\n\n\nQ5.8\n\n\nQ5.9\n\n\nQ5.10\n\n\nQ5.11\n\n\nQ5.12\n\n\nQ5.13\n\n\nQ5.14\n\n\nQ5.15\n\n\nQ5.16",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Visualization of Multivariate Data</span>"
    ]
  },
  {
    "objectID": "chapters/06_mcvar.html",
    "href": "chapters/06_mcvar.html",
    "title": "6  Monte Carlo Integration and Variance Reduction",
    "section": "",
    "text": "Q6.1\n\n\nQ6.2\n\n\nQ6.3\n\n\nQ6.4\n\n\nQ6.5\n\n\nQ6.6\n\n\nQ6.7\n\n\nQ6.8\n\n\nQ6.9\n\n\nQ6.10\n\n\nQ6.11\n\n\nQ6.12\n\n\nQ6.13\n\n\nQ6.14\n\n\nQ6.15",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Monte Carlo Integration and Variance Reduction</span>"
    ]
  },
  {
    "objectID": "chapters/07_mcinf.html",
    "href": "chapters/07_mcinf.html",
    "title": "7  Monte Carlo Methods in Inference",
    "section": "",
    "text": "Q7.1\n\n\nQ7.2\n\n\nQ7.3\n\n\nQ7.4\n\n\nQ7.5\n\n\nQ7.6\n\n\nQ7.7\n\n\nQ7.8\n\n\nQ7.9\n\n\nQ7.10",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Monte Carlo Methods in Inference</span>"
    ]
  },
  {
    "objectID": "chapters/08_bootjack.html",
    "href": "chapters/08_bootjack.html",
    "title": "8  Bootstrap and Jackknife",
    "section": "",
    "text": "Q8.1\n\n\nQ8.2\n\n\nQ8.3\n\n\nQ8.4\n\n\nQ8.5\n\n\nQ8.6\n\n\nQ8.7\n\n\nQ8.8\n\n\nQ8.9\n\n\nQ8.10\n\n\nQ8.11",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Bootstrap and Jackknife</span>"
    ]
  },
  {
    "objectID": "chapters/09_resamp.html",
    "href": "chapters/09_resamp.html",
    "title": "9  Resamping Applications",
    "section": "",
    "text": "Q9.1\n\n\nQ9.2\n\n\nQ9.3\n\n\nQ9.4\n\n\nQ9.5\n\n\nQ9.6\n\n\nQ9.7",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Resamping Applications</span>"
    ]
  },
  {
    "objectID": "chapters/10_permute.html",
    "href": "chapters/10_permute.html",
    "title": "10  Permutation Tests",
    "section": "",
    "text": "Q10.1\n\n\nQ10.2\n\n\nQ10.3\n\n\nQ10.4\n\n\nQ10.5\n\n\nQ10.6\n\n\nQ10.7",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Permutation Tests</span>"
    ]
  },
  {
    "objectID": "chapters/11_mcmc.html",
    "href": "chapters/11_mcmc.html",
    "title": "11  Markov Chain Monte Carlo Methods",
    "section": "",
    "text": "Q11.1\n\n\nQ11.2\n\n\nQ11.3\n\n\nQ11.4\n\n\nQ11.5\n\n\nQ11.6\n\n\nQ11.7\n\n\nQ11.8\n\n\nQ11.9\n\n\nQ11.10\n\n\nQ11.11\n\n\nQ11.12\n\n\nQ11.13\n\n\nQ11.14\n\n\nQ11.15",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Markov Chain Monte Carlo Methods</span>"
    ]
  },
  {
    "objectID": "chapters/12_densityest.html",
    "href": "chapters/12_densityest.html",
    "title": "12  Probability Density Estimation",
    "section": "",
    "text": "Q12.1\n\n\nQ12.2\n\n\nQ12.3\n\n\nQ12.4\n\n\nQ12.5\n\n\nQ12.6\n\n\nQ12.7\n\n\nQ12.8\n\n\nQ12.9\n\n\nQ12.10\n\n\nQ12.11\n\n\nQ12.12\n\n\nQ12.13",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Probability Density Estimation</span>"
    ]
  },
  {
    "objectID": "chapters/13_numerical.html",
    "href": "chapters/13_numerical.html",
    "title": "13  Introduction to Numerical Methods in R",
    "section": "",
    "text": "Q13.1\n\n\nQ13.2\n\n\nQ13.3\n\n\nQ13.4\n\n\nQ13.5\n\n\nQ13.6",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Introduction to Numerical Methods in R</span>"
    ]
  },
  {
    "objectID": "chapters/14_optim.html",
    "href": "chapters/14_optim.html",
    "title": "14  Optimization",
    "section": "",
    "text": "Q14.1\n\n\nQ14.2",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Optimization</span>"
    ]
  },
  {
    "objectID": "chapters/15_program.html",
    "href": "chapters/15_program.html",
    "title": "15  Programming Topics",
    "section": "",
    "text": "Q15.1\n\n\nQ15.2\n\n\nQ15.3\n\n\nQ15.4\n\n\nQ15.5\n\n\nQ15.6",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Programming Topics</span>"
    ]
  }
]